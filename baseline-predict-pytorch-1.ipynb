{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This preliminary notebook using some code form https://www.kaggle.com/mrinath/efficientdet-train-pytorch","metadata":{}},{"cell_type":"code","source":"# Try doing all the installations here\n!pip install -I numpy\n\n!pip install -I torchvision\n!pip install -I torch -U   \n\n# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n\n!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -I 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' --no-binary pycocotools\n!ls /kaggle/input/baseline-predict-pytorch\n\n# # !pip install -I pycocotools==2.0.0\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:10:42.417301Z","iopub.execute_input":"2022-01-31T12:10:42.417554Z","iopub.status.idle":"2022-01-31T12:13:35.124447Z","shell.execute_reply.started":"2022-01-31T12:10:42.417484Z","shell.execute_reply":"2022-01-31T12:13:35.123673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls /kaggle/input/baseline-predict-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:58:41.14513Z","iopub.execute_input":"2022-01-30T07:58:41.149318Z","iopub.status.idle":"2022-01-30T07:58:41.155953Z","shell.execute_reply.started":"2022-01-30T07:58:41.149268Z","shell.execute_reply":"2022-01-30T07:58:41.154446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cp -r /kaggle/input/cococode/PythonAPI/pycocotools/ /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:58:41.159495Z","iopub.execute_input":"2022-01-30T07:58:41.160765Z","iopub.status.idle":"2022-01-30T07:58:41.168688Z","shell.execute_reply.started":"2022-01-30T07:58:41.160722Z","shell.execute_reply":"2022-01-30T07:58:41.167639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python /kaggle/input/cococode/PythonAPI/setup.py  build_ext install","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:58:41.173274Z","iopub.execute_input":"2022-01-30T07:58:41.174886Z","iopub.status.idle":"2022-01-30T07:58:41.183069Z","shell.execute_reply.started":"2022-01-30T07:58:41.174842Z","shell.execute_reply":"2022-01-30T07:58:41.181861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pycocotools._mask as mask","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:58:41.186008Z","iopub.execute_input":"2022-01-30T07:58:41.186259Z","iopub.status.idle":"2022-01-30T07:58:41.20255Z","shell.execute_reply.started":"2022-01-30T07:58:41.186228Z","shell.execute_reply":"2022-01-30T07:58:41.198689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# git clone the utility functions and evaluation functions from pytorch coco dataset\n\n!git clone https://github.com/pytorch/vision.git\n!cd vision\n!git checkout v0.8.2","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:14:42.132137Z","iopub.execute_input":"2022-01-31T12:14:42.132567Z","iopub.status.idle":"2022-01-31T12:15:02.007763Z","shell.execute_reply.started":"2022-01-31T12:14:42.132527Z","shell.execute_reply":"2022-01-31T12:15:02.006928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls vision/references","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:59:04.73515Z","iopub.execute_input":"2022-01-30T07:59:04.735602Z","iopub.status.idle":"2022-01-30T07:59:05.498704Z","shell.execute_reply.started":"2022-01-30T07:59:04.735539Z","shell.execute_reply":"2022-01-30T07:59:05.497529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp vision/references/detection/utils.py /kaggle/working/\n!cp vision/references/detection/transforms.py /kaggle/working/\n!cp vision/references/detection/coco_eval.py /kaggle/working/\n!cp vision/references/detection/engine.py /kaggle/working/\n!cp vision/references/detection/coco_utils.py /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:15:28.233516Z","iopub.execute_input":"2022-01-31T12:15:28.233963Z","iopub.status.idle":"2022-01-31T12:15:31.59736Z","shell.execute_reply.started":"2022-01-31T12:15:28.233929Z","shell.execute_reply":"2022-01-31T12:15:31.596499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nimport sys\nimport os\n\n# os.environ['TORCH_HOME'] = '\\\\kaggle\\\\input\\\\resnet'\n\nimport glob\nimport sklearn\nimport math\nimport random\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics, model_selection, preprocessing\nfrom sklearn.model_selection import GroupKFold\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-31T12:17:37.331858Z","iopub.execute_input":"2022-01-31T12:17:37.332392Z","iopub.status.idle":"2022-01-31T12:17:43.856961Z","shell.execute_reply.started":"2022-01-31T12:17:37.332353Z","shell.execute_reply":"2022-01-31T12:17:43.856214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train only on images with detections let see\ndf = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\ndf = df[df.annotations != '[]']\ndf = df.reset_index(drop = True)\ndf.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:18:46.339349Z","iopub.execute_input":"2022-01-31T12:18:46.339999Z","iopub.status.idle":"2022-01-31T12:18:46.418151Z","shell.execute_reply.started":"2022-01-31T12:18:46.339958Z","shell.execute_reply":"2022-01-31T12:18:46.417342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['fold'] = -1\nkf = GroupKFold(n_splits = 5)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)):\n    df.loc[val_idx, 'fold'] = fold","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:20:18.66381Z","iopub.execute_input":"2022-01-31T12:20:18.664072Z","iopub.status.idle":"2022-01-31T12:20:18.677592Z","shell.execute_reply.started":"2022-01-31T12:20:18.664044Z","shell.execute_reply":"2022-01-31T12:20:18.676879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:20:21.926812Z","iopub.execute_input":"2022-01-31T12:20:21.927074Z","iopub.status.idle":"2022-01-31T12:20:21.941184Z","shell.execute_reply.started":"2022-01-31T12:20:21.927046Z","shell.execute_reply":"2022-01-31T12:20:21.940397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.fold.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:20:25.055935Z","iopub.execute_input":"2022-01-31T12:20:25.056194Z","iopub.status.idle":"2022-01-31T12:20:25.066078Z","shell.execute_reply.started":"2022-01-31T12:20:25.056166Z","shell.execute_reply":"2022-01-31T12:20:25.065344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the imaging paths to the dataframe\ndf['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\ndf['annotations'] = df['annotations'].apply(eval)\ndf['number_boxes'] = df['annotations'].apply(lambda x: len(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:20:42.371615Z","iopub.execute_input":"2022-01-31T12:20:42.371881Z","iopub.status.idle":"2022-01-31T12:20:42.54763Z","shell.execute_reply.started":"2022-01-31T12:20:42.37185Z","shell.execute_reply":"2022-01-31T12:20:42.546842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot some of the images\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\ndef get_rectangle_edges_from_pascal_bbox(bbox):\n    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n\n    bottom_left = (xmin_top_left, ymax_bottom_right)\n    width = xmax_bottom_right - xmin_top_left\n    height = ymin_top_left - ymax_bottom_right\n\n    return bottom_left, width, height\n\ndef draw_pascal_voc_bboxes(\n    plot_ax,\n    bboxes,\n    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n):\n    for bbox in bboxes:\n        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n\n        rect_1 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"black\",\n            fill=False,\n        )\n        rect_2 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"red\",\n            fill=False,\n        )\n\n        # Add the patch to the Axes\n        plot_ax.add_patch(rect_1)\n        plot_ax.add_patch(rect_2)\n\ndef draw_image(\n    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n):\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(image)\n\n    if bboxes is not None:\n        draw_bboxes_fn(ax, bboxes)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:04.970715Z","iopub.execute_input":"2022-01-31T12:21:04.97102Z","iopub.status.idle":"2022-01-31T12:21:04.979507Z","shell.execute_reply.started":"2022-01-31T12:21:04.970989Z","shell.execute_reply":"2022-01-31T12:21:04.978859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataAdaptor:\n    def __init__(self,df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n    \n    def get_image_bb(self , idx):\n        img_src = self.df.loc[idx,'path']\n        image   = cv2.imread(img_src)\n        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        row     = self.df.iloc[idx]\n        bboxes  = self.get_boxes(row) \n        class_labels = np.ones(len(bboxes))\n        return image, bboxes, class_labels, idx\n    \n        \n    def show_image(self, index):\n        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n        print(f\"image_id: {image_id}\")\n        draw_image(image, bboxes.tolist())\n#         print(class_labels) \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:12.509237Z","iopub.execute_input":"2022-01-31T12:21:12.509525Z","iopub.status.idle":"2022-01-31T12:21:12.519403Z","shell.execute_reply.started":"2022-01-31T12:21:12.509494Z","shell.execute_reply":"2022-01-31T12:21:12.518371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = DataAdaptor(df)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:15.669945Z","iopub.execute_input":"2022-01-31T12:21:15.670191Z","iopub.status.idle":"2022-01-31T12:21:15.67391Z","shell.execute_reply.started":"2022-01-31T12:21:15.670163Z","shell.execute_reply":"2022-01-31T12:21:15.67324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im,bb,_,_ = train_ds.get_image_bb(4005)\nbb","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:18.372262Z","iopub.execute_input":"2022-01-31T12:21:18.372953Z","iopub.status.idle":"2022-01-31T12:21:18.446015Z","shell.execute_reply.started":"2022-01-31T12:21:18.372914Z","shell.execute_reply":"2022-01-31T12:21:18.445328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_ds.show_image(2016)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:21.200924Z","iopub.execute_input":"2022-01-31T12:21:21.201397Z","iopub.status.idle":"2022-01-31T12:21:21.677244Z","shell.execute_reply.started":"2022-01-31T12:21:21.201354Z","shell.execute_reply":"2022-01-31T12:21:21.676615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.where(df[\"number_boxes\"] > 2)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:25.19467Z","iopub.execute_input":"2022-01-31T12:21:25.195196Z","iopub.status.idle":"2022-01-31T12:21:25.202144Z","shell.execute_reply.started":"2022-01-31T12:21:25.195154Z","shell.execute_reply":"2022-01-31T12:21:25.201487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_seq = [len(df[df['video_id'] == i]) for i in range(3)]\nlabels = [\"0\", \"1\", \"2\"]\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\nax.set_facecolor('aliceblue')\nplt.grid(color=\"gray\", linestyle=\"-\", zorder=0)\nplt.ylabel(\"Number of Frames\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Video ID\", fontsize=16, fontweight=\"bold\")\nplt.title(\"Length of train videos\", fontsize=20, fontweight=\"bold\")\nplt.bar(labels, num_seq, color=\"orange\", zorder=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:29.825213Z","iopub.execute_input":"2022-01-31T12:21:29.825502Z","iopub.status.idle":"2022-01-31T12:21:30.005106Z","shell.execute_reply.started":"2022-01-31T12:21:29.825468Z","shell.execute_reply":"2022-01-31T12:21:30.004312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_num = max(df.number_boxes)\nmax_sample = df[df[\"number_boxes\"] == max_num].sample()\nmax_vid_id = max_sample.video_id.values[0]\nmax_vid_frame = max_sample.video_frame.values[0]\n\nprint('\\033[1m' + f\"Maximum number of starfish in one frame: {max_num} (Video {max_vid_id}, Frame {max_vid_frame})\" + '\\033[0m')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:34.587047Z","iopub.execute_input":"2022-01-31T12:21:34.587822Z","iopub.status.idle":"2022-01-31T12:21:34.597007Z","shell.execute_reply.started":"2022-01-31T12:21:34.587784Z","shell.execute_reply":"2022-01-31T12:21:34.596117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_ds.show_image(max_sample.index[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:37.761733Z","iopub.execute_input":"2022-01-31T12:21:37.762565Z","iopub.status.idle":"2022-01-31T12:21:38.241081Z","shell.execute_reply.started":"2022-01-31T12:21:37.762512Z","shell.execute_reply":"2022-01-31T12:21:38.240293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check number of samples without boxes\nmin_num = 0\nmin_sample = df[df[\"number_boxes\"] == 0]\nprint(len(min_sample), len(df), len(df)-len(min_sample))","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:21:41.689818Z","iopub.execute_input":"2022-01-31T12:21:41.690092Z","iopub.status.idle":"2022-01-31T12:21:41.696439Z","shell.execute_reply.started":"2022-01-31T12:21:41.690061Z","shell.execute_reply":"2022-01-31T12:21:41.695488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\n\nclass CotsData(torch.utils.data.Dataset):\n    def __init__(self, df, transforms=None):\n        self.ds = df\n        self.transforms = transforms\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n            \n    def __getitem__(self, idx):\n        # load images\n        img_path = self.ds.loc[idx,'path']\n        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        \n        row = self.ds.iloc[idx]\n        boxes = self.get_boxes(row)\n        num_objs = self.ds.loc[idx, 'number_boxes']\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:22:07.748416Z","iopub.execute_input":"2022-01-31T12:22:07.749156Z","iopub.status.idle":"2022-01-31T12:22:07.763813Z","shell.execute_reply.started":"2022-01-31T12:22:07.749119Z","shell.execute_reply":"2022-01-31T12:22:07.763069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #pretrained=True\n#     model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n#     # now get the number of input features for the mask classifier\n#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n#     hidden_layer = 256\n#     # and replace the mask predictor with a new one\n#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n#                                                        hidden_layer,\n#                                                        num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:22:15.108447Z","iopub.execute_input":"2022-01-31T12:22:15.109174Z","iopub.status.idle":"2022-01-31T12:22:15.11588Z","shell.execute_reply.started":"2022-01-31T12:22:15.109136Z","shell.execute_reply":"2022-01-31T12:22:15.114945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/tensorflow-great-barrier-reef","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:59:20.026048Z","iopub.execute_input":"2022-01-30T07:59:20.027393Z","iopub.status.idle":"2022-01-30T07:59:20.857489Z","shell.execute_reply.started":"2022-01-30T07:59:20.027331Z","shell.execute_reply":"2022-01-30T07:59:20.85635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_npy = np.load(\"/kaggle/input/tensorflow-great-barrier-reef/example_test.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:59:20.864602Z","iopub.execute_input":"2022-01-30T07:59:20.864894Z","iopub.status.idle":"2022-01-30T07:59:21.20768Z","shell.execute_reply.started":"2022-01-30T07:59:20.864861Z","shell.execute_reply":"2022-01-30T07:59:21.206679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:59:21.209276Z","iopub.execute_input":"2022-01-30T07:59:21.209723Z","iopub.status.idle":"2022-01-30T07:59:21.223397Z","shell.execute_reply.started":"2022-01-30T07:59:21.209666Z","shell.execute_reply":"2022-01-30T07:59:21.22228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subdf = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/example_sample_submission.csv\")\nsubdf","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:59:21.225185Z","iopub.execute_input":"2022-01-30T07:59:21.225638Z","iopub.status.idle":"2022-01-30T07:59:21.242649Z","shell.execute_reply.started":"2022-01-30T07:59:21.22559Z","shell.execute_reply":"2022-01-30T07:59:21.241453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat transforms.py","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:22:54.189456Z","iopub.execute_input":"2022-01-31T12:22:54.189763Z","iopub.status.idle":"2022-01-31T12:22:54.893581Z","shell.execute_reply.started":"2022-01-31T12:22:54.189729Z","shell.execute_reply":"2022-01-31T12:22:54.892657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/tensorflow-great-barrier-reef/greatbarrierreef/","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:59:22.028835Z","iopub.execute_input":"2022-01-30T07:59:22.029156Z","iopub.status.idle":"2022-01-30T07:59:22.811528Z","shell.execute_reply.started":"2022-01-30T07:59:22.029112Z","shell.execute_reply":"2022-01-30T07:59:22.810412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n        transforms.append(T.RandomPhotometricDistort())\n        transforms.append(T.RandomZoomOut())\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:28:47.224279Z","iopub.execute_input":"2022-01-31T12:28:47.224944Z","iopub.status.idle":"2022-01-31T12:28:47.254222Z","shell.execute_reply.started":"2022-01-31T12:28:47.224905Z","shell.execute_reply":"2022-01-31T12:28:47.253484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_n = 1\ntrain_df= df[df.fold != fold_n]\nval_df  = df[df.fold == fold_n]\n\n# use our dataset and defined transformations\ndataset = CotsData(train_df.reset_index(drop=True), get_transform(train=True))\ndataset_test = CotsData(val_df.reset_index(drop=True), get_transform(train=False))","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:29:12.195707Z","iopub.execute_input":"2022-01-31T12:29:12.196485Z","iopub.status.idle":"2022-01-31T12:29:12.211065Z","shell.execute_reply.started":"2022-01-31T12:29:12.196417Z","shell.execute_reply":"2022-01-31T12:29:12.210264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset in train and test set\ntorch.manual_seed(1)\n# indices = torch.randperm(len(dataset)).tolist()\n# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=4, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:29:23.903994Z","iopub.execute_input":"2022-01-31T12:29:23.904649Z","iopub.status.idle":"2022-01-31T12:29:23.912195Z","shell.execute_reply.started":"2022-01-31T12:29:23.904611Z","shell.execute_reply":"2022-01-31T12:29:23.911498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:29:46.008493Z","iopub.execute_input":"2022-01-31T12:29:46.008786Z","iopub.status.idle":"2022-01-31T12:29:58.536294Z","shell.execute_reply.started":"2022-01-31T12:29:46.008758Z","shell.execute_reply":"2022-01-31T12:29:58.535558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skip this cell if we only want to load the already trained model\n# let's train it for 10 epochs\n# no training\nfrom torch.optim.lr_scheduler import StepLR\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\ntorch.save(model.state_dict(), 'checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:30:11.586112Z","iopub.execute_input":"2022-01-31T12:30:11.586362Z","iopub.status.idle":"2022-01-31T15:20:23.152637Z","shell.execute_reply.started":"2022-01-31T12:30:11.586333Z","shell.execute_reply":"2022-01-31T15:20:23.151712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PATH = 'checkpoint.pth'\n# torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'loss': loss,\n#             }, PATH)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.726958Z","iopub.execute_input":"2022-01-30T10:21:42.727582Z","iopub.status.idle":"2022-01-30T10:21:42.733452Z","shell.execute_reply.started":"2022-01-30T10:21:42.727528Z","shell.execute_reply":"2022-01-30T10:21:42.732411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'../working')\nfrom IPython.display import FileLink\nFileLink(r'checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T15:24:14.439265Z","iopub.execute_input":"2022-01-31T15:24:14.439942Z","iopub.status.idle":"2022-01-31T15:24:14.44799Z","shell.execute_reply.started":"2022-01-31T15:24:14.439879Z","shell.execute_reply":"2022-01-31T15:24:14.447286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_path = '/kaggle/input/savemodel/checkpoint.pth'","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.81827Z","iopub.execute_input":"2022-01-30T10:21:42.818722Z","iopub.status.idle":"2022-01-30T10:21:42.826033Z","shell.execute_reply.started":"2022-01-30T10:21:42.818672Z","shell.execute_reply":"2022-01-30T10:21:42.825122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# state_dict = torch.load(model_path)\n# # print(state_dict.keys())\n# model.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.827728Z","iopub.execute_input":"2022-01-30T10:21:42.828138Z","iopub.status.idle":"2022-01-30T10:21:42.845321Z","shell.execute_reply.started":"2022-01-30T10:21:42.828092Z","shell.execute_reply":"2022-01-30T10:21:42.844433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.35):\n    \n#     # torchvision returns the indices of the bboxes to keep\n#     # function to implement non maximm suppression\n#     # might also need to eliminate predictions with very low scores\n#     # trim low scores first\n    \n#     keep = orig_prediction['scores'] >= score_thresh\n    \n#     scores_prediction = {}\n#     scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n#     scores_prediction['scores'] = orig_prediction['scores'][keep]\n#     scores_prediction['labels'] = orig_prediction['labels'][keep]\n    \n#     keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n    \n#     final_prediction = {}\n#     final_prediction['boxes'] = scores_prediction['boxes'][keep]\n#     final_prediction['scores'] = scores_prediction['scores'][keep]\n#     final_prediction['labels'] = scores_prediction['labels'][keep]\n    \n#     return final_prediction\n\n# def return_predict_string(predictions):\n#     str_p = ''\n#     for i, score in enumerate(predictions['scores']):\n#         box = predictions['boxes'][i].cpu()\n#         str_p += f'{score} {int(np.round(box[0]))} {int(np.round(box[1]))} {int(np.round(box[2]-box[0]))} {int(np.round(box[3]-box[1]))} '\n    \n#     str_p = str_p.strip(' ')\n#     if str_p == '':\n#         str_p = '0.9 716 678 54 42'\n    \n#     return str_p\n\n# def preprocess_img(img):\n#     img = img/255.\n#     x,y, c = img.shape\n#     img = img.reshape(c,x,y)\n#     return torch.from_numpy(img)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.847405Z","iopub.execute_input":"2022-01-30T10:21:42.847811Z","iopub.status.idle":"2022-01-30T10:21:42.858409Z","shell.execute_reply.started":"2022-01-30T10:21:42.847749Z","shell.execute_reply":"2022-01-30T10:21:42.857234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # pick one image from the test set\n# img, target = dataset_test[5]\n# # put the model in evaluation mode\n# model.eval()\n# with torch.no_grad():\n#     prediction = model([img.to(device=device, dtype=torch.float)])[0]\n#     final_pred = apply_nms(prediction, 0.2)\n    \n# print('predicted #boxes: ', len(prediction['labels']))\n# print('real #boxes: ', len(target['labels']))\n# print('nms predict #boxes: ', len(final_pred['labels']))\n# print('scores: ', prediction['scores'])\n# print(return_predict_string(prediction))\n\n# print(prediction)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.860558Z","iopub.execute_input":"2022-01-30T10:21:42.860986Z","iopub.status.idle":"2022-01-30T10:21:42.909078Z","shell.execute_reply.started":"2022-01-30T10:21:42.860933Z","shell.execute_reply":"2022-01-30T10:21:42.907873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import greatbarrierreef\n# rows=[]\n# ii = 0\n# env = greatbarrierreef.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n# for (pixel_array, sample_prediction_df) in iter_test:\n#     pixel_p = preprocess_img(pixel_array)\n#     prediction = model([pixel_p.to(device, dtype=torch.float)])[0]\n#     sample_prediction_df['annotations'] = anno = '0.5 0 0 100 100' #return_predict_string(apply_nms(prediction, 0.3))  # make your predictions here\n#     rows.append([ii, anno])\n#     env.predict(sample_prediction_df)\n#     ii += 1\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.911328Z","iopub.execute_input":"2022-01-30T10:21:42.911771Z","iopub.status.idle":"2022-01-30T10:21:42.922942Z","shell.execute_reply.started":"2022-01-30T10:21:42.91169Z","shell.execute_reply":"2022-01-30T10:21:42.921924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rows\n# model([pixel_p.to(device=device, dtype=torch.float)])[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.924448Z","iopub.execute_input":"2022-01-30T10:21:42.924834Z","iopub.status.idle":"2022-01-30T10:21:42.933532Z","shell.execute_reply.started":"2022-01-30T10:21:42.924787Z","shell.execute_reply":"2022-01-30T10:21:42.932654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_ds = DataAdaptor(val_df.reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.935538Z","iopub.execute_input":"2022-01-30T10:21:42.935887Z","iopub.status.idle":"2022-01-30T10:21:42.945157Z","shell.execute_reply.started":"2022-01-30T10:21:42.935843Z","shell.execute_reply":"2022-01-30T10:21:42.944068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img5 = test_ds.show_image(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.947179Z","iopub.execute_input":"2022-01-30T10:21:42.947935Z","iopub.status.idle":"2022-01-30T10:21:42.955253Z","shell.execute_reply.started":"2022-01-30T10:21:42.947842Z","shell.execute_reply":"2022-01-30T10:21:42.95419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_p*255.","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.956634Z","iopub.execute_input":"2022-01-30T10:21:42.957233Z","iopub.status.idle":"2022-01-30T10:21:42.966419Z","shell.execute_reply.started":"2022-01-30T10:21:42.957185Z","shell.execute_reply":"2022-01-30T10:21:42.96533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(pixel_p.reshape(720,1280,3))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:21:42.967912Z","iopub.execute_input":"2022-01-30T10:21:42.968481Z","iopub.status.idle":"2022-01-30T10:21:42.976799Z","shell.execute_reply.started":"2022-01-30T10:21:42.968436Z","shell.execute_reply":"2022-01-30T10:21:42.975837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}