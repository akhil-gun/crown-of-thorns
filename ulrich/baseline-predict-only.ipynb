{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This preliminary notebook using some code form https://www.kaggle.com/mrinath/efficientdet-train-pytorch","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/k/ulricharmel/baseline-predict-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:26:57.127376Z","iopub.execute_input":"2022-02-03T04:26:57.127898Z","iopub.status.idle":"2022-02-03T04:26:57.812496Z","shell.execute_reply.started":"2022-02-03T04:26:57.127798Z","shell.execute_reply":"2022-02-03T04:26:57.811667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/utils.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/transforms.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/coco_eval.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/engine.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/coco_utils.py /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:26:57.81446Z","iopub.execute_input":"2022-02-03T04:26:57.814938Z","iopub.status.idle":"2022-02-03T04:27:01.131786Z","shell.execute_reply.started":"2022-02-03T04:26:57.814899Z","shell.execute_reply":"2022-02-03T04:27:01.130704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot some of the images\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\ndef get_rectangle_edges_from_pascal_bbox(bbox):\n    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n\n    bottom_left = (xmin_top_left, ymax_bottom_right)\n    width = xmax_bottom_right - xmin_top_left\n    height = ymin_top_left - ymax_bottom_right\n\n    return bottom_left, width, height\n\ndef draw_pascal_voc_bboxes(\n    plot_ax,\n    bboxes,\n    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n):\n    for bbox in bboxes:\n        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n\n        rect_1 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"black\",\n            fill=False,\n        )\n        rect_2 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"red\",\n            fill=False,\n        )\n\n        # Add the patch to the Axes\n        plot_ax.add_patch(rect_1)\n        plot_ax.add_patch(rect_2)\n\ndef draw_image(\n    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n):\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(image)\n\n    if bboxes is not None:\n        draw_bboxes_fn(ax, bboxes)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:01.134482Z","iopub.execute_input":"2022-02-03T04:27:01.134696Z","iopub.status.idle":"2022-02-03T04:27:01.145298Z","shell.execute_reply.started":"2022-02-03T04:27:01.134667Z","shell.execute_reply":"2022-02-03T04:27:01.14446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataAdaptor:\n    def __init__(self,df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n    \n    def get_image_bb(self , idx):\n        img_src = self.df.loc[idx,'path']\n        image   = cv2.imread(img_src)\n        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        row     = self.df.iloc[idx]\n        bboxes  = self.get_boxes(row) \n        class_labels = np.ones(len(bboxes))\n        return image, bboxes, class_labels, idx\n    \n        \n    def show_image(self, index):\n        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n        print(f\"image_id: {image_id}\")\n        draw_image(image, bboxes.tolist())\n#         print(class_labels) \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:01.147663Z","iopub.execute_input":"2022-02-03T04:27:01.148384Z","iopub.status.idle":"2022-02-03T04:27:01.161424Z","shell.execute_reply.started":"2022-02-03T04:27:01.148346Z","shell.execute_reply":"2022-02-03T04:27:01.160542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\n\nclass CotsData(torch.utils.data.Dataset):\n    def __init__(self, df, transforms=None):\n        self.ds = df\n        self.transforms = transforms\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n            \n    def __getitem__(self, idx):\n        # load images\n        img_path = self.ds.loc[idx,'path']\n        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        \n        row = self.ds.iloc[idx]\n        boxes = self.get_boxes(row)\n        num_objs = self.ds.loc[idx, 'number_boxes']\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:01.162879Z","iopub.execute_input":"2022-02-03T04:27:01.163346Z","iopub.status.idle":"2022-02-03T04:27:01.72334Z","shell.execute_reply.started":"2022-02-03T04:27:01.16325Z","shell.execute_reply":"2022-02-03T04:27:01.72257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False) #pretrained=True\n    model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n#     # now get the number of input features for the mask classifier\n#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n#     hidden_layer = 256\n#     # and replace the mask predictor with a new one\n#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n#                                                        hidden_layer,\n#                                                        num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:01.724674Z","iopub.execute_input":"2022-02-03T04:27:01.724908Z","iopub.status.idle":"2022-02-03T04:27:01.732043Z","shell.execute_reply.started":"2022-02-03T04:27:01.724874Z","shell.execute_reply":"2022-02-03T04:27:01.731148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from engine import train_one_epoch, evaluate\n# import utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:01.733347Z","iopub.execute_input":"2022-02-03T04:27:01.733784Z","iopub.status.idle":"2022-02-03T04:27:01.747666Z","shell.execute_reply.started":"2022-02-03T04:27:01.733746Z","shell.execute_reply":"2022-02-03T04:27:01.746836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we redifine the model\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:01.749283Z","iopub.execute_input":"2022-02-03T04:27:01.749551Z","iopub.status.idle":"2022-02-03T04:27:04.224083Z","shell.execute_reply.started":"2022-02-03T04:27:01.749517Z","shell.execute_reply":"2022-02-03T04:27:04.223371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# This loads the model\nmodel_path = '/kaggle/input/checkvideo2/checkpoint-video2.pth'\nstate_dict = torch.load(model_path)\n# print(state_dict.keys())\nmodel.load_state_dict(state_dict)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:04.225368Z","iopub.execute_input":"2022-02-03T04:27:04.225775Z","iopub.status.idle":"2022-02-03T04:27:04.394366Z","shell.execute_reply.started":"2022-02-03T04:27:04.225736Z","shell.execute_reply":"2022-02-03T04:27:04.393613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.35):\n    \n    # torchvision returns the indices of the bboxes to keep\n    # function to implement non maximm suppression\n    # might also need to eliminate predictions with very low scores\n    # trim low scores first\n    \n    keep = orig_prediction['scores'] >= score_thresh\n    \n    scores_prediction = {}\n    scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n    scores_prediction['scores'] = orig_prediction['scores'][keep]\n    scores_prediction['labels'] = orig_prediction['labels'][keep]\n    \n    keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n    \n    final_prediction = {}\n    final_prediction['boxes'] = scores_prediction['boxes'][keep]\n    final_prediction['scores'] = scores_prediction['scores'][keep]\n    final_prediction['labels'] = scores_prediction['labels'][keep]\n    \n    return final_prediction\n\ndef return_predict_string(predictions):\n    str_p = ''\n    for i, score in enumerate(predictions['scores']):\n        box = predictions['boxes'][i].cpu().detach().numpy()\n        score = score.cpu().detach().numpy()\n        str_p += f'{score:.3f} {int(np.round(box[0]))} {int(np.round(box[1]))} {int(np.round(box[2]-box[0]))} {int(np.round(box[3]-box[1]))} '\n    \n    str_p = str_p.strip(' ')\n#     if str_p == '':\n#         str_p = '0.9 716 678 54 42'\n    \n    return str_p\n\ndef preprocess_img(img):\n    img = img/255.\n    x,y, c = img.shape\n    img = img.transpose(2, 0, 1)\n    return torch.from_numpy(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:04.397281Z","iopub.execute_input":"2022-02-03T04:27:04.398053Z","iopub.status.idle":"2022-02-03T04:27:04.408671Z","shell.execute_reply.started":"2022-02-03T04:27:04.398021Z","shell.execute_reply":"2022-02-03T04:27:04.407834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nrows=[]\nii = 0\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (pixel_array, sample_prediction_df) in iter_test:\n    pixel_p = preprocess_img(pixel_array)\n    prediction = model([pixel_p.to(device, dtype=torch.float)])[0]\n    sample_prediction_df['annotations'] = anno = return_predict_string(apply_nms(prediction, 0, 0))  # make your predictions here\n    rows.append([ii, anno])\n    env.predict(sample_prediction_df)\n    ii += 1\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:04.410127Z","iopub.execute_input":"2022-02-03T04:27:04.410435Z","iopub.status.idle":"2022-02-03T04:27:10.676718Z","shell.execute_reply.started":"2022-02-03T04:27:04.410384Z","shell.execute_reply":"2022-02-03T04:27:10.675955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows\n# model([pixel_p.to(device=device, dtype=torch.float)])[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.678107Z","iopub.execute_input":"2022-02-03T04:27:10.678369Z","iopub.status.idle":"2022-02-03T04:27:10.685142Z","shell.execute_reply.started":"2022-02-03T04:27:10.678334Z","shell.execute_reply":"2022-02-03T04:27:10.684329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls /kaggle/input/tensorflow-great-barrier-reef/","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.686793Z","iopub.execute_input":"2022-02-03T04:27:10.687379Z","iopub.status.idle":"2022-02-03T04:27:10.693408Z","shell.execute_reply.started":"2022-02-03T04:27:10.68734Z","shell.execute_reply":"2022-02-03T04:27:10.692635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = np.load(\"/kaggle/input/tensorflow-great-barrier-reef/example_test.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.69489Z","iopub.execute_input":"2022-02-03T04:27:10.6955Z","iopub.status.idle":"2022-02-03T04:27:10.702357Z","shell.execute_reply.started":"2022-02-03T04:27:10.695438Z","shell.execute_reply":"2022-02-03T04:27:10.701605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_p = preprocess_img(data[0])\n# prediction = model([pixel_p.to(device, dtype=torch.float)])[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.70383Z","iopub.execute_input":"2022-02-03T04:27:10.704373Z","iopub.status.idle":"2022-02-03T04:27:10.711585Z","shell.execute_reply.started":"2022-02-03T04:27:10.704336Z","shell.execute_reply":"2022-02-03T04:27:10.710848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.714365Z","iopub.execute_input":"2022-02-03T04:27:10.714605Z","iopub.status.idle":"2022-02-03T04:27:10.720624Z","shell.execute_reply.started":"2022-02-03T04:27:10.714564Z","shell.execute_reply":"2022-02-03T04:27:10.720011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_path = '/kaggle/input/tensorflow-great-barrier-reef/train_images/video_0/12142.jpg'\n# img = Image.open(img_path).convert(\"RGB\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.722319Z","iopub.execute_input":"2022-02-03T04:27:10.72264Z","iopub.status.idle":"2022-02-03T04:27:10.731301Z","shell.execute_reply.started":"2022-02-03T04:27:10.722602Z","shell.execute_reply":"2022-02-03T04:27:10.730583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img2 = convert_tensor(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.734209Z","iopub.execute_input":"2022-02-03T04:27:10.734776Z","iopub.status.idle":"2022-02-03T04:27:10.740495Z","shell.execute_reply.started":"2022-02-03T04:27:10.734735Z","shell.execute_reply":"2022-02-03T04:27:10.739732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img2[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.741831Z","iopub.execute_input":"2022-02-03T04:27:10.742404Z","iopub.status.idle":"2022-02-03T04:27:10.74944Z","shell.execute_reply.started":"2022-02-03T04:27:10.74236Z","shell.execute_reply":"2022-02-03T04:27:10.748688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction = model([img2[0].to(device)])[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.750372Z","iopub.execute_input":"2022-02-03T04:27:10.752419Z","iopub.status.idle":"2022-02-03T04:27:10.75815Z","shell.execute_reply.started":"2022-02-03T04:27:10.752372Z","shell.execute_reply":"2022-02-03T04:27:10.757427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.759091Z","iopub.execute_input":"2022-02-03T04:27:10.759505Z","iopub.status.idle":"2022-02-03T04:27:10.767812Z","shell.execute_reply.started":"2022-02-03T04:27:10.759449Z","shell.execute_reply":"2022-02-03T04:27:10.767026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_p.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.769048Z","iopub.execute_input":"2022-02-03T04:27:10.769834Z","iopub.status.idle":"2022-02-03T04:27:10.777379Z","shell.execute_reply.started":"2022-02-03T04:27:10.769779Z","shell.execute_reply":"2022-02-03T04:27:10.776565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_p*255.","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.778673Z","iopub.execute_input":"2022-02-03T04:27:10.779286Z","iopub.status.idle":"2022-02-03T04:27:10.786867Z","shell.execute_reply.started":"2022-02-03T04:27:10.779248Z","shell.execute_reply":"2022-02-03T04:27:10.786172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(pixel_p.reshape(720,1280,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T04:27:10.78812Z","iopub.execute_input":"2022-02-03T04:27:10.788712Z","iopub.status.idle":"2022-02-03T04:27:10.795688Z","shell.execute_reply.started":"2022-02-03T04:27:10.788667Z","shell.execute_reply":"2022-02-03T04:27:10.79503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}