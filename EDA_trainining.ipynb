{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11c2de9",
   "metadata": {},
   "source": [
    "## Exploratory data analysis and training\n",
    "\n",
    "This notebook describes the steps of my exploratory data analyis and training with pytorch.\n",
    "The training is done by finetunning a Fast RCNN model using pytocrch. I followed this tutorial https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html for the purpose. \n",
    "\n",
    "Also I copied some code from this notebook to quickly familiarise myself with understanding the dataset.\n",
    "https://www.kaggle.com/mrinath/efficientdet-train-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934a039",
   "metadata": {},
   "source": [
    "### Setting up my training environment\n",
    "\n",
    "Since I am using pytorch and finetunning a FastRCNN (https://arxiv.org/pdf/1506.01497.pdf), I need to install\n",
    "and download certain files for my data analysis. Unfortunately, I had few dependecies issues among the different\n",
    "packages so I had to reinstall most on kaggle (because kaggle actually have a lot of these packages already install).\n",
    "\n",
    "If you don't want to run my model and you only want to familiarise your self with understanding the data, you can skip these first few cells untill the ones below EDA section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeca2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try doing all the installations here\n",
    "!pip install -I numpy\n",
    "\n",
    "!pip install -I torchvision\n",
    "!pip install -I torch -U   \n",
    "\n",
    "# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n",
    "\n",
    "!pip install cython\n",
    "# Install pycocotools, the version by default in Colab\n",
    "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
    "!pip install -I 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' --no-binary pycocotools\n",
    "# !ls /kaggle/input/baseline-predict-pytorch\n",
    "\n",
    "# !pip install -I pycocotools==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone the utility functions and evaluation functions from pytorch coco dataset\n",
    "\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!cd vision\n",
    "!git checkout v0.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c428a5",
   "metadata": {},
   "source": [
    "Note that in kaggle all your input files will be in a directory /kaggle/input/\n",
    "\n",
    "So you need to do !ls /kaggle/input/ if you want to list the files in this directory.\n",
    "\n",
    "All the output files generated by your notebook are stored in /kaggle/output/\n",
    "\n",
    "If you to use external scripts in your notebook, you have to put them in /kaggle/working. \n",
    "\n",
    "So after git clone vision from pytorch, I copy the folling files to /kaggle/working/ because \n",
    "I will need them during my training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp vision/references/detection/utils.py /kaggle/working/\n",
    "!cp vision/references/detection/transforms.py /kaggle/working/\n",
    "!cp vision/references/detection/coco_eval.py /kaggle/working/\n",
    "!cp vision/references/detection/engine.py /kaggle/working/\n",
    "!cp vision/references/detection/coco_utils.py /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92307aea",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# os.environ['TORCH_HOME'] = '\\\\kaggle\\\\input\\\\resnet'\n",
    "\n",
    "import glob\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Make sure to select GPU when you are training on kaggle if not you might need to run your model\n",
    "# ages\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train csv with pandas\n",
    "df = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\n",
    "# df = df[df.annotations != '[]'] # in my next training I will activate this to drop images with no starfish\n",
    "# df = df.reset_index(drop = True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f418829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just assigning different folds to the data. This basically is just to help if you want to cross validation\n",
    "# google cross validation if you don't know what that means\n",
    "df['fold'] = -1\n",
    "kf = GroupKFold(n_splits = 5)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)):\n",
    "    df.loc[val_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ca682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the imaging paths to the dataframe\n",
    "df['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\n",
    "df['annotations'] = df['annotations'].apply(eval)\n",
    "df['number_boxes'] = df['annotations'].apply(lambda x: len(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a52c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the images\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "def get_rectangle_edges_from_pascal_bbox(bbox):\n",
    "    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n",
    "\n",
    "    bottom_left = (xmin_top_left, ymax_bottom_right)\n",
    "    width = xmax_bottom_right - xmin_top_left\n",
    "    height = ymin_top_left - ymax_bottom_right\n",
    "\n",
    "    return bottom_left, width, height\n",
    "\n",
    "def draw_pascal_voc_bboxes(\n",
    "    plot_ax,\n",
    "    bboxes,\n",
    "    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n",
    "):\n",
    "    for bbox in bboxes:\n",
    "        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n",
    "\n",
    "        rect_1 = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"black\",\n",
    "            fill=False,\n",
    "        )\n",
    "        rect_2 = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"red\",\n",
    "            fill=False,\n",
    "        )\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        plot_ax.add_patch(rect_1)\n",
    "        plot_ax.add_patch(rect_2)\n",
    "\n",
    "def draw_image(\n",
    "    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n",
    "):\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if bboxes is not None:\n",
    "        draw_bboxes_fn(ax, bboxes)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAdaptor:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_boxes(self, row):\n",
    "        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n",
    "        \n",
    "        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n",
    "        \n",
    "        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n",
    "        # if you check you will see the images have shape 1280 by 720\n",
    "        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n",
    "        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n",
    "        \n",
    "        return boxes\n",
    "    \n",
    "    def get_image_bb(self , idx):\n",
    "        img_src = self.df.loc[idx,'path']\n",
    "        image   = cv2.imread(img_src)\n",
    "        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        row     = self.df.iloc[idx]\n",
    "        bboxes  = self.get_boxes(row) \n",
    "        class_labels = np.ones(len(bboxes))\n",
    "        return image, bboxes, class_labels, idx\n",
    "    \n",
    "        \n",
    "    def show_image(self, index):\n",
    "        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n",
    "        print(f\"image_id: {image_id}\")\n",
    "        draw_image(image, bboxes.tolist())\n",
    "#         print(class_labels) \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd841d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataAdaptor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a70773",
   "metadata": {},
   "outputs": [],
   "source": [
    "im,bb,_,_ = train_ds.get_image_bb(4005)\n",
    "#bb are the position of the boxes in this image.\n",
    "#These what we trying to predict\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_ds.show_image(20163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6329b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(df[\"number_boxes\"] > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of images in each video folder\n",
    "\n",
    "num_seq = [len(df[df['video_id'] == i]) for i in range(3)]\n",
    "labels = [\"0\", \"1\", \"2\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "ax.set_facecolor('aliceblue')\n",
    "plt.grid(color=\"gray\", linestyle=\"-\", zorder=0)\n",
    "plt.ylabel(\"Number of Frames\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Video ID\", fontsize=16, fontweight=\"bold\")\n",
    "plt.title(\"Length of train videos\", fontsize=20, fontweight=\"bold\")\n",
    "plt.bar(labels, num_seq, color=\"orange\", zorder=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = max(df.number_boxes)\n",
    "max_sample = df[df[\"number_boxes\"] == max_num].sample()\n",
    "max_vid_id = max_sample.video_id.values[0]\n",
    "max_vid_frame = max_sample.video_frame.values[0]\n",
    "\n",
    "print('\\033[1m' + f\"Maximum number of starfish in one frame: {max_num} (Video {max_vid_id}, Frame {max_vid_frame})\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_ds.show_image(max_sample.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of samples without boxes\n",
    "min_num = 0\n",
    "min_sample = df[df[\"number_boxes\"] == 0]\n",
    "print(len(min_sample), len(df), len(df)-len(min_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0276c6",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "This is the part where I define and train the model with pytroch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2745b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CotsData(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.ds = df\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def get_boxes(self, row):\n",
    "        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n",
    "        \n",
    "        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n",
    "        \n",
    "        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n",
    "        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n",
    "        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n",
    "        \n",
    "        return boxes\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # load images\n",
    "        img_path = self.ds.loc[idx,'path']\n",
    "        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        \n",
    "        row = self.ds.iloc[idx]\n",
    "        boxes = self.get_boxes(row)\n",
    "        num_objs = self.ds.loc[idx, 'number_boxes']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # during predict I will use this instead\n",
    "#     model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "#     model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "# I can use this function do define different types of augmentations\n",
    "# I want to apply to the data\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a36da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now I will not do any cross validation\n",
    "# I will just train the model and use the first fold, fold_n = 0 as validation data set\n",
    "\n",
    "fold_n = 0\n",
    "train_df= df[df.fold != fold_n]\n",
    "val_df  = df[df.fold == fold_n]\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = CotsData(train_df.reset_index(drop=True), get_transform(train=True))\n",
    "dataset_test = CotsData(val_df.reset_index(drop=True), get_transform(train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b801664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "# # indices = torch.randperm(len(dataset)).tolist()\n",
    "# # dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# # dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "# we need to use a small batch size if not the we may run out of memory\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and starfish\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeca7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will train and save the model\n",
    "# let's train it for 1 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, target = dataset_test[5]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device=device, dtype=torch.float)])[0]\n",
    "    \n",
    "print('predicted #boxes: ', len(prediction['labels']))\n",
    "print('real #boxes: ', len(target['labels']))\n",
    "print('scores: ', prediction['scores'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
