{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddc2cb9",
   "metadata": {},
   "source": [
    "### Predicting and submitting\n",
    "\n",
    "For this competion we need to submit a notebook. They will run the notebook and get the submission csv.\n",
    "\n",
    "All the notebook must be ran without internet, so we don't need to install or download anything when running this note book. So we have do down and add everything we need as datasets to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this assumes you added the these files in a folder called baseline-predict-pytorch\n",
    "!cp /kaggle/input/baseline-predict-pytorch/utils.py /kaggle/working/\n",
    "!cp /kaggle/input/baseline-predict-pytorch/transforms.py /kaggle/working/\n",
    "!cp /kaggle/input/baseline-predict-pytorch/coco_eval.py /kaggle/working/\n",
    "!cp /kaggle/input/baseline-predict-pytorch/engine.py /kaggle/working/\n",
    "!cp /kaggle/input/baseline-predict-pytorch/coco_utils.py /kaggle/working/\n",
    "\n",
    "# Also I need to add this model as the data set in order to be able to define my model.\n",
    "!ls /kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
    "\n",
    "# finally this is the model I saved from the other model\n",
    "!ls /kaggle/input/savemodel/checkpoint.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CotsData(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.ds = df\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def get_boxes(self, row):\n",
    "        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n",
    "        \n",
    "        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n",
    "        \n",
    "        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n",
    "        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n",
    "        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n",
    "        \n",
    "        return boxes\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # load images\n",
    "        img_path = self.ds.loc[idx,'path']\n",
    "        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        \n",
    "        row = self.ds.iloc[idx]\n",
    "        boxes = self.get_boxes(row)\n",
    "        num_objs = self.ds.loc[idx, 'number_boxes']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a00186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "    model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from engine import train_one_epoch, evaluate\n",
    "# import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26914396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we redifine the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)"
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b919982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the model\n",
    "model_path = '/kaggle/input/savemodel/checkpoint.pth'\n",
    "state_dict = torch.load(model_path)\n",
    "# print(state_dict.keys())\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e16f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.35):\n",
    "    \n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    # function to implement non maximm suppression\n",
    "    # might also need to eliminate predictions with very low scores\n",
    "    # trim low scores first\n",
    "    \n",
    "    keep = orig_prediction['scores'] >= score_thresh\n",
    "    \n",
    "    scores_prediction = {}\n",
    "    scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n",
    "    scores_prediction['scores'] = orig_prediction['scores'][keep]\n",
    "    scores_prediction['labels'] = orig_prediction['labels'][keep]\n",
    "    \n",
    "    keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n",
    "    \n",
    "    final_prediction = {}\n",
    "    final_prediction['boxes'] = scores_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = scores_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = scores_prediction['labels'][keep]\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "\n",
    "# so far I haven't had success formatting the output correctly with this function\n",
    "def return_predict_string(predictions):\n",
    "    str_p = ''\n",
    "    for i, score in enumerate(predictions['scores']):\n",
    "        box = predictions['boxes'][i].cpu()\n",
    "        str_p += f'{score} {int(np.round(box[0]))} {int(np.round(box[1]))} {int(np.round(box[2]-box[0]))} {int(np.round(box[3]-box[1]))} '\n",
    "    \n",
    "    str_p = str_p.strip(' ')\n",
    "    if str_p == '':\n",
    "        str_p = '0.9 716 678 54 42'\n",
    "    \n",
    "    return str_p\n",
    "\n",
    "# I have to reshaoe the arrays before predicting and also scale\n",
    "def preprocess_img(img):\n",
    "    img = img/255.\n",
    "    x,y, c = img.shape\n",
    "    img = img.reshape(c,x,y)\n",
    "    return torch.from_numpy(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does the submission\n",
    "# note I am not passing the output from the model for now\n",
    "import greatbarrierreef\n",
    "rows=[]\n",
    "ii = 0\n",
    "env = greatbarrierreef.make_env()   # initialize the environment\n",
    "iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n",
    "for (pixel_array, sample_prediction_df) in iter_test:\n",
    "    pixel_p = preprocess_img(pixel_array)\n",
    "    prediction = model([pixel_p.to(device, dtype=torch.float)])[0]\n",
    "    sample_prediction_df['annotations'] = anno = '0.5 0 0 100 100' #return_predict_string(apply_nms(prediction, 0.3))  # make your predictions here\n",
    "    rows.append([ii, anno])\n",
    "    env.predict(sample_prediction_df)\n",
    "    ii += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
